{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Processing Data in spark because it massive",
   "id": "9afdc14d78d87dbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import config\n",
    "\n",
    "# Take caution, running this on local would probably crash\n",
    "spark = (\n",
    "        SparkSession.builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"Word Count\")\n",
    "       .config(\"spark.some.config.option\", \"some-value\")\n",
    "       .getOrCreate()\n",
    ")"
   ],
   "id": "abff6068ca6a0245"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = spark.read.csv(config.DATA_PATH, header=True, multiLine=True, escape='\"', mode=\"DROPMALFORMED\")\n",
    "# Clean column names\n",
    "for col in df.columns:\n",
    "    clean_name = col.strip().replace('\"', '')\n",
    "    df = df.withColumnRenamed(col, clean_name)"
   ],
   "id": "6c25f36f4b1052a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = df.select(\n",
    "    F.trim(F.col(\"playlistname\")).alias(\"playlistname\"),\n",
    "    F.trim(F.col(\"trackname\")).alias(\"trackname\"),\n",
    "    F.trim(F.col(\"artistname\")).alias(\"artistname\")\n",
    ")\n",
    "\n",
    "# Clean text data and combine track+artist\n",
    "df = df.withColumn(\"trackname\", F.lower(F.col(\"trackname\"))) \\\n",
    "       .withColumn(\"artistname\", F.lower(F.col(\"artistname\"))) \\\n",
    "       .withColumn(\"track_artist\", F.concat(F.col(\"trackname\"), F.lit(\" by \"), F.col(\"artistname\")))\n",
    "\n",
    "# Group by playlist and filter\n",
    "playlist_df = df.groupBy(\"playlistname\") \\\n",
    "                .agg(F.collect_list(\"track_artist\").alias(\"tracklist\")) \\\n",
    "                .filter(F.size(\"tracklist\") > 40)\n",
    "playlist_df.cache()"
   ],
   "id": "89dce5972cc6ed1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get unique songs and compute embeddings\n",
    "unique_songs = playlist_df.select(F.explode(\"tracklist\").alias(\"song\")).distinct()\n",
    "\n",
    "# Define schema for embedding UDF\n",
    "embedding_schema = ArrayType(FloatType())\n",
    "\n",
    "@pandas_udf(embedding_schema)\n",
    "def encode_songs_udf(songs: pd.Series) -> pd.Series:\n",
    "    \"\"\"Pandas UDF to encode songs in parallel across workers\"\"\"\n",
    "    # Load model on each worker (this happens once per worker)\n",
    "    encoder = SentenceTransformer(config.EMBEDDING_MODEL, device='cpu')\n",
    "\n",
    "    # Encode all songs in this partition\n",
    "    embeddings = encoder.encode(songs.tolist(), convert_to_numpy=True)\n",
    "\n",
    "    # Return as pandas Series\n",
    "    return pd.Series([embedding for embedding in embeddings])\n",
    "\n",
    "# Add partition ID to help with distribution\n",
    "unique_songs = unique_songs.repartition(64)  # Adjust based on cluster size\n",
    "embeddings_df = unique_songs.withColumn(\"embedding\", encode_songs_udf(\"song\"))\n",
    "embeddings_df.cache()\n",
    "# embeddings_df.write.parquet(config.EMBEDDINGS_FILE_PATH, mode=\"overwrite\")\n",
    "# embeddings_df = spark.read.json('/Volumes/mkopa_default/testing_brackly_murunga/generative_modelling/song_embeddings.parquet/')"
   ],
   "id": "eb41c54798d04696"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Process playlists in distributed fashion\n",
    "processed_playlists = playlist_df.withColumn(\n",
    "    \"chunks\",\n",
    "    F.transform(\n",
    "        F.sequence(F.lit(0), F.floor(F.size(\"tracklist\") / config.CONTEXT_SIZE) - 1),\n",
    "        lambda i: F.slice(\"tracklist\", i * config.CONTEXT_SIZE + 1, config.CONTEXT_SIZE)\n",
    "    )\n",
    ").withColumn(\"split_index\",\n",
    "    F.floor(F.size(\"chunks\") * F.lit(config.TRAIN_RATIO)).cast(\"int\")\n",
    ").withColumn(\"val_index\",\n",
    "    F.floor(F.size(\"chunks\") * F.lit(config.TRAIN_RATIO + config.VAL_RATIO)).cast(\"int\")\n",
    ").withColumn(\"playlist_id\", F.monotonically_increasing_id())\n",
    "\n",
    "\n",
    "# Split into train/val/test and add chunk IDs\n",
    "train_df = processed_playlists.withColumn(\n",
    "    \"train_chunks\",\n",
    "    F.slice(\"chunks\", 1, F.col(\"split_index\"))\n",
    ").select(\n",
    "    'playlist_id',\n",
    "    F.monotonically_increasing_id().alias(\"chunk_id\"),\n",
    "    F.explode(\"train_chunks\").alias(\"chunk\")\n",
    ")\n",
    "\n",
    "val_df = processed_playlists.withColumn(\n",
    "    \"val_chunks\",\n",
    "    F.slice(\"chunks\", F.col(\"split_index\") + 1, F.col(\"val_index\") - F.col(\"split_index\"))\n",
    ").select(\n",
    "    'playlist_id',\n",
    "    F.monotonically_increasing_id().alias(\"chunk_id\"),\n",
    "    F.explode(\"val_chunks\").alias(\"chunk\")\n",
    ")\n",
    "\n",
    "test_df = processed_playlists.withColumn(\n",
    "    \"test_chunks\",\n",
    "    F.slice(\"chunks\", F.col(\"val_index\") + 1, F.size(\"chunks\") - F.col(\"val_index\"))\n",
    ").select(\n",
    "    'playlist_id',\n",
    "    F.monotonically_increasing_id().alias(\"chunk_id\"),\n",
    "    F.explode(\"test_chunks\").alias(\"chunk\")\n",
    ")"
   ],
   "id": "c66a7318317969be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to process chunks using joins instead of broadcasting\n",
    "def process_chunks_with_join(chunk_df, dataset_name):\n",
    "    # Explode chunks to get individual songs with their positions\n",
    "    exploded_chunks = chunk_df.select(\n",
    "        \"chunk_id\",'playlist_id',\n",
    "        F.posexplode(\"chunk\").alias(\"pos\", \"song\")\n",
    "    )\n",
    "\n",
    "    # Join with embeddings\n",
    "    chunks_with_embeddings = exploded_chunks.join(\n",
    "        embeddings_df,\n",
    "        exploded_chunks.song == embeddings_df.song,\n",
    "        \"inner\"\n",
    "    ).drop(embeddings_df.song).select(\n",
    "        \"chunk_id\", \"pos\", \"song\",'playlist_id',\n",
    "        F.col(\"embedding\").alias(\"embedding\")\n",
    "    )\n",
    "\n",
    "    # Group by chunk_id and collect embeddings in order\n",
    "    grouped_embeddings = chunks_with_embeddings.groupBy(\"chunk_id\",\"playlist_id\").agg(\n",
    "        F.sort_array(\n",
    "            F.collect_list(F.struct(\"pos\", \"embedding\"))\n",
    "        ).alias(\"sorted_embeddings\")\n",
    "    ).select(\n",
    "        \"chunk_id\",\n",
    "        \"playlist_id\",\n",
    "        F.expr(\"transform(sorted_embeddings, x -> x.embedding)\").alias(\"embeddings\")\n",
    "    )\n",
    "\n",
    "    # Write to storage\n",
    "    output_path = f\"{config.OUTPUT_PATH}{dataset_name}\"\n",
    "    grouped_embeddings.write.parquet(output_path, mode=\"overwrite\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# Process each dataset\n",
    "train_path = process_chunks_with_join(train_df, \"train\")\n",
    "val_path = process_chunks_with_join(val_df, \"val\")\n",
    "test_path = process_chunks_with_join(test_df, \"test\")\n",
    "\n",
    "print(f\"Train data written to: {train_path}\")\n",
    "print(f\"Validation data written to: {val_path}\")\n",
    "print(f\"Test data written to: {test_path}\")"
   ],
   "id": "3fa270a6a91fa2ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:24:54.072173Z",
     "start_time": "2025-09-07T20:24:45.696346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from playgenie.data.dataset import DataSet\n",
    "import pandas as pd"
   ],
   "id": "38fd025a7d7ec1e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:24:54.124440Z",
     "start_time": "2025-09-07T20:24:54.085633Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = DataSet(folder_path='../data/train')",
   "id": "3031a4cca51dc246",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:25:59.155736Z",
     "start_time": "2025-09-07T20:25:57.919817Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.__getitem__(0)",
   "id": "81e0fa0f502270f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0772,  0.0288,  0.0500,  ..., -0.0355,  0.0366, -0.1130],\n",
       "          [-0.0372, -0.0300,  0.0429,  ...,  0.0395,  0.0329, -0.0317],\n",
       "          [-0.0850, -0.0648,  0.0652,  ...,  0.1156, -0.0064, -0.1484],\n",
       "          ...,\n",
       "          [-0.0985, -0.0101,  0.0528,  ..., -0.0148,  0.0127, -0.0121],\n",
       "          [-0.1077,  0.0797, -0.0016,  ...,  0.0873,  0.0091,  0.0331],\n",
       "          [-0.0233, -0.0761,  0.0458,  ...,  0.0778,  0.0160, -0.0382]],\n",
       " \n",
       "         [[ 0.0056,  0.0249,  0.0208,  ..., -0.0228, -0.0080, -0.0405],\n",
       "          [ 0.0704, -0.0093, -0.0020,  ...,  0.0013, -0.0194, -0.0466],\n",
       "          [ 0.0622, -0.0403,  0.0346,  ..., -0.0404, -0.0292, -0.0221],\n",
       "          ...,\n",
       "          [ 0.0125,  0.0291, -0.0080,  ...,  0.0115,  0.0351, -0.0085],\n",
       "          [ 0.0200,  0.0072,  0.0095,  ..., -0.0159,  0.0245, -0.0097],\n",
       "          [ 0.0778, -0.0394,  0.0447,  ..., -0.0478,  0.0475, -0.0673]],\n",
       " \n",
       "         [[-0.0039,  0.0017,  0.0329,  ..., -0.0278, -0.0586,  0.0134],\n",
       "          [-0.0625,  0.0518,  0.0438,  ...,  0.1009, -0.0353,  0.0262],\n",
       "          [-0.0470,  0.0082,  0.0067,  ..., -0.0389, -0.0104,  0.0274],\n",
       "          ...,\n",
       "          [-0.0322,  0.0541, -0.0382,  ...,  0.0846, -0.0038, -0.0398],\n",
       "          [-0.0399, -0.0255,  0.0307,  ...,  0.0803,  0.0268, -0.0713],\n",
       "          [-0.0021,  0.0361,  0.0014,  ...,  0.0729,  0.0148, -0.0219]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0353,  0.0515, -0.0311,  ...,  0.0707, -0.0120,  0.0250],\n",
       "          [-0.0572,  0.0621, -0.1187,  ...,  0.0412, -0.0527, -0.0709],\n",
       "          [-0.0515,  0.0229, -0.0386,  ...,  0.0242, -0.1022, -0.0418],\n",
       "          ...,\n",
       "          [-0.0611,  0.0667,  0.0421,  ...,  0.1006,  0.0365, -0.0184],\n",
       "          [-0.0444,  0.0835,  0.0740,  ...,  0.0266,  0.0775, -0.0324],\n",
       "          [-0.1067,  0.0343, -0.0251,  ...,  0.0384,  0.0077,  0.0014]],\n",
       " \n",
       "         [[-0.0724,  0.0330, -0.0373,  ...,  0.0360,  0.0080, -0.0746],\n",
       "          [-0.0877,  0.0267,  0.0008,  ...,  0.0421, -0.0268, -0.1763],\n",
       "          [-0.0373,  0.0754, -0.0430,  ..., -0.0252,  0.0566, -0.1049],\n",
       "          ...,\n",
       "          [-0.0828, -0.0466, -0.0192,  ...,  0.0383, -0.0876,  0.0032],\n",
       "          [-0.0586, -0.0357,  0.0005,  ...,  0.0164,  0.0578, -0.0177],\n",
       "          [-0.0874,  0.0846, -0.0093,  ..., -0.0350,  0.0510, -0.0491]],\n",
       " \n",
       "         [[-0.0467,  0.0204, -0.1001,  ..., -0.0043,  0.0035, -0.0348],\n",
       "          [-0.0501, -0.0168, -0.1069,  ...,  0.0086, -0.0484, -0.0219],\n",
       "          [-0.0107, -0.0194, -0.0185,  ...,  0.0141,  0.1706, -0.0184],\n",
       "          ...,\n",
       "          [-0.0785,  0.0725, -0.0023,  ...,  0.0626, -0.0568,  0.0390],\n",
       "          [ 0.0347,  0.0054, -0.0044,  ...,  0.0104, -0.0306, -0.0100],\n",
       "          [-0.0267, -0.0036,  0.0177,  ..., -0.0327, -0.0656,  0.0691]]]),\n",
       " tensor([[[-3.7198e-02, -2.9995e-02,  4.2945e-02,  ...,  3.9512e-02,\n",
       "            3.2864e-02, -3.1679e-02],\n",
       "          [-8.5014e-02, -6.4837e-02,  6.5200e-02,  ...,  1.1561e-01,\n",
       "           -6.4225e-03, -1.4835e-01],\n",
       "          [-2.5998e-02,  2.8548e-02,  3.0156e-02,  ...,  5.7138e-02,\n",
       "           -1.6596e-02, -5.0184e-02],\n",
       "          ...,\n",
       "          [-1.0774e-01,  7.9719e-02, -1.6115e-03,  ...,  8.7282e-02,\n",
       "            9.1249e-03,  3.3130e-02],\n",
       "          [-2.3319e-02, -7.6139e-02,  4.5848e-02,  ...,  7.7846e-02,\n",
       "            1.5957e-02, -3.8170e-02],\n",
       "          [-1.5355e-01,  6.8270e-02,  8.3793e-02,  ...,  5.6322e-02,\n",
       "           -4.8811e-03,  1.2357e-02]],\n",
       " \n",
       "         [[ 7.0408e-02, -9.3181e-03, -1.9957e-03,  ...,  1.2731e-03,\n",
       "           -1.9421e-02, -4.6643e-02],\n",
       "          [ 6.2197e-02, -4.0308e-02,  3.4583e-02,  ..., -4.0374e-02,\n",
       "           -2.9222e-02, -2.2092e-02],\n",
       "          [ 5.1009e-02, -2.0829e-02,  1.8610e-02,  ..., -1.3330e-02,\n",
       "            1.7291e-02, -2.6203e-02],\n",
       "          ...,\n",
       "          [ 2.0012e-02,  7.2127e-03,  9.4879e-03,  ..., -1.5896e-02,\n",
       "            2.4494e-02, -9.6718e-03],\n",
       "          [ 7.7841e-02, -3.9443e-02,  4.4676e-02,  ..., -4.7818e-02,\n",
       "            4.7511e-02, -6.7327e-02],\n",
       "          [ 3.3648e-02, -2.3146e-02,  5.4926e-02,  ..., -9.6085e-03,\n",
       "            7.0752e-02, -7.3253e-02]],\n",
       " \n",
       "         [[-6.2460e-02,  5.1752e-02,  4.3761e-02,  ...,  1.0087e-01,\n",
       "           -3.5350e-02,  2.6181e-02],\n",
       "          [-4.6966e-02,  8.2411e-03,  6.7482e-03,  ..., -3.8919e-02,\n",
       "           -1.0424e-02,  2.7426e-02],\n",
       "          [-4.7187e-02, -7.2634e-02,  3.1505e-02,  ..., -1.9799e-02,\n",
       "           -4.1919e-03,  2.4349e-02],\n",
       "          ...,\n",
       "          [-3.9869e-02, -2.5462e-02,  3.0670e-02,  ...,  8.0280e-02,\n",
       "            2.6761e-02, -7.1301e-02],\n",
       "          [-2.1329e-03,  3.6133e-02,  1.3838e-03,  ...,  7.2907e-02,\n",
       "            1.4794e-02, -2.1920e-02],\n",
       "          [ 2.5084e-02, -2.5773e-02,  8.1032e-02,  ...,  2.2387e-02,\n",
       "           -1.5836e-02, -9.4328e-03]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-5.7161e-02,  6.2143e-02, -1.1870e-01,  ...,  4.1196e-02,\n",
       "           -5.2724e-02, -7.0853e-02],\n",
       "          [-5.1544e-02,  2.2943e-02, -3.8581e-02,  ...,  2.4160e-02,\n",
       "           -1.0224e-01, -4.1824e-02],\n",
       "          [-8.8731e-02,  5.6706e-02,  1.7334e-02,  ...,  2.3558e-02,\n",
       "           -8.9759e-02,  1.2989e-02],\n",
       "          ...,\n",
       "          [-4.4361e-02,  8.3497e-02,  7.3953e-02,  ...,  2.6648e-02,\n",
       "            7.7518e-02, -3.2398e-02],\n",
       "          [-1.0674e-01,  3.4334e-02, -2.5080e-02,  ...,  3.8358e-02,\n",
       "            7.7167e-03,  1.4417e-03],\n",
       "          [-6.4242e-02,  8.3192e-02,  7.4288e-03,  ...,  2.3747e-02,\n",
       "           -3.2416e-02,  2.8458e-02]],\n",
       " \n",
       "         [[-8.7718e-02,  2.6669e-02,  7.6213e-04,  ...,  4.2140e-02,\n",
       "           -2.6796e-02, -1.7631e-01],\n",
       "          [-3.7255e-02,  7.5375e-02, -4.2993e-02,  ..., -2.5237e-02,\n",
       "            5.6564e-02, -1.0493e-01],\n",
       "          [-6.5724e-02,  2.0387e-02, -6.7680e-02,  ...,  3.8845e-02,\n",
       "            5.9898e-02,  2.5290e-03],\n",
       "          ...,\n",
       "          [-5.8632e-02, -3.5711e-02,  5.1946e-04,  ...,  1.6402e-02,\n",
       "            5.7823e-02, -1.7672e-02],\n",
       "          [-8.7443e-02,  8.4637e-02, -9.2709e-03,  ..., -3.5039e-02,\n",
       "            5.1035e-02, -4.9065e-02],\n",
       "          [ 8.6353e-03,  4.4098e-05,  5.0879e-03,  ...,  4.4430e-02,\n",
       "           -8.1133e-02, -1.5180e-02]],\n",
       " \n",
       "         [[-5.0079e-02, -1.6833e-02, -1.0688e-01,  ...,  8.6040e-03,\n",
       "           -4.8437e-02, -2.1880e-02],\n",
       "          [-1.0737e-02, -1.9396e-02, -1.8536e-02,  ...,  1.4120e-02,\n",
       "            1.7060e-01, -1.8418e-02],\n",
       "          [-6.5642e-02, -6.8216e-02,  3.6633e-02,  ...,  9.5351e-02,\n",
       "           -1.6832e-02,  2.8957e-02],\n",
       "          ...,\n",
       "          [ 3.4693e-02,  5.3825e-03, -4.3893e-03,  ...,  1.0388e-02,\n",
       "           -3.0587e-02, -9.9674e-03],\n",
       "          [-2.6714e-02, -3.5707e-03,  1.7735e-02,  ..., -3.2698e-02,\n",
       "           -6.5571e-02,  6.9079e-02],\n",
       "          [-7.6152e-02, -1.1059e-01,  2.7936e-03,  ..., -7.4022e-03,\n",
       "            5.5928e-02,  8.4594e-02]]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
